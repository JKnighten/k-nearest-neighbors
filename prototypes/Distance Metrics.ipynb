{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Distance/Similarity Metrics\n",
    "Although many packages contain implementation of many distance/similarity metrics, we will still implement them by hand to gain more familiarity with them. The only external package we will use is numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean Distance or L2-Norm\n",
    "Euclidean distance is probably the most widely taught distance metric. It taught in early on in school to find the distance between two points on a coordinate grid. It is a simple application of pythagorean theorem in two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean Distance: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Create Two Simple Vectors(Distance 1)\n",
    "vector_1 = np.array([0, 0])\n",
    "vector_2 = np.array([0, 1])\n",
    "\n",
    "# Find Difference(Aka Created Vector Between The Two Vectors)\n",
    "difference = vector_1-vector_2\n",
    "\n",
    "# Square Each Number In Vector And Sum Them Up\n",
    "squared = np.dot(difference, difference)\n",
    "\n",
    "# Find Square Root Of Sum Of Squares\n",
    "euclidean_distance = np.sqrt(squared)\n",
    "\n",
    "print(\"Euclidean Distance: \" + str(euclidean_distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean Distance: 1.0\n",
      "Euclidean Distance: 3.0000000000000004\n"
     ]
    }
   ],
   "source": [
    "# Create Function\n",
    "def euclidean(vector_1, vector_2):\n",
    "    difference = vector_1-vector_2\n",
    "    distance = np.sqrt(np.dot(difference, difference))\n",
    "    return distance\n",
    "\n",
    "# On Unit Vector\n",
    "vector_1 = np.array([0, 0])\n",
    "vector_2 = np.array([math.sqrt(2)/2, math.sqrt(2)/2])\n",
    "print(\"Euclidean Distance: \" + str(euclidean(vector_1, vector_2)))\n",
    "\n",
    "# Scaled From Above Example(3 Times Length)\n",
    "vector_1 = np.array([0, 0])\n",
    "vector_2 = np.array([3*math.sqrt(2)/2, 3*math.sqrt(2)/2])\n",
    "print(\"Euclidean Distance: \" + str(euclidean(vector_1, vector_2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manhattan Distance or L1-Norm or City-Block Distance\n",
    "Manhattan distance is a simple to calculate distance metric. It is simply the sum of the absolute differences of the two vectors being compared. Unlike euclidean distance which is the measurement of the straight line between two points, manhattan is the sum of the difference in each axis. An analogy is traveling through a city whose roads are in a grid. For instance, you would tell somone to walk north 3 blocks and east 2 blocks meaning they would travel 5 blocks total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manhattan Distance: 1\n"
     ]
    }
   ],
   "source": [
    "# Create Two Simple Vectors(Distance 1)\n",
    "vector_1 = np.array([0, 0])\n",
    "vector_2 = np.array([0, 1])\n",
    "\n",
    "# Find Difference(Aka Created Vector Between The Two Vectors)\n",
    "difference = vector_1-vector_2\n",
    "\n",
    "absolute_diff = np.abs(difference)\n",
    "\n",
    "manhattan_distance = np.sum(absolute_diff)\n",
    "\n",
    "print(\"Manhattan Distance: \" + str(manhattan_distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manhattan Distance: 1.4142135623730951\n",
      "Manhattan Distance: 4.242640687119286\n"
     ]
    }
   ],
   "source": [
    "# Create Function\n",
    "def manhattan(vector_1, vector_2):\n",
    "    difference = vector_1-vector_2\n",
    "    absolute_diff = np.abs(difference)\n",
    "    distance = np.sum(absolute_diff)\n",
    "    return distance\n",
    "\n",
    "# On Unit Vector\n",
    "vector_1 = np.array([0, 0])\n",
    "vector_2 = np.array([math.sqrt(2)/2, math.sqrt(2)/2])\n",
    "print(\"Manhattan Distance: \" + str(manhattan(vector_1, vector_2)))\n",
    "\n",
    "# Scaled From Above Example(3 Times Length)\n",
    "vector_1 = np.array([0, 0])\n",
    "vector_2 = np.array([3*math.sqrt(2)/2, 3*math.sqrt(2)/2])\n",
    "print(\"Manhattan Distance: \" + str(manhattan(vector_1, vector_2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similairty\n",
    "Cosine similarity is a measurement that can be used as a distance metric, but it is not a proper distance metric since it violates the triangle inequality. Cosine similarity is simply the cosine of the angle between two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Johnny/Code/k-nearest-neighbors/knn_env/lib/python3.5/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in true_divide\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# Create Two Simple Vectors(Distance 1)\n",
    "vector_1 = np.array([0, 0])\n",
    "vector_2 = np.array([0, 1])\n",
    "\n",
    "dot_prob = np.dot(vector_1, vector_2)\n",
    "vector_1_norm = np.sqrt(np.dot(vector_1, vector_1))\n",
    "vector_2_norm = np.sqrt(np.dot(vector_2, vector_2))\n",
    "\n",
    "cosine_sim = dot_prob/(vector_1_norm*vector_2_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here is the first \"gotcha\" with cosine similarity. If we have a vector with a L2-Norm(dot product with itself) that is zero, then we will end up with a divide by zero error. So we will add an if statement to catch that case. There is no possible value in this situation so we will return NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: nan\n"
     ]
    }
   ],
   "source": [
    "dot_prob = np.dot(vector_1, vector_2)\n",
    "vector_1_norm = np.sqrt(np.dot(vector_1, vector_1))\n",
    "vector_2_norm = np.sqrt(np.dot(vector_2, vector_2))\n",
    "\n",
    "if vector_1_norm ==0 or vector_2_norm==0:\n",
    "    cosine_sim = np.nan\n",
    "else: \n",
    "    cosine_sim = dot_prob/(vector_1_norm*vector_2_norm)\n",
    "    \n",
    "print(\"Cosine Similarity: \" + str(cosine_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.7071067811865476\n",
      "cos(45 Degrees):0.7071067811865476\n",
      "Cosine Similarity: 0.0\n",
      "cos(90 Degrees):6.123233995736766e-17\n"
     ]
    }
   ],
   "source": [
    "# Create Function\n",
    "def cosine(vector_1, vector_2):\n",
    "    dot_prob = np.dot(vector_1, vector_2)\n",
    "    vector_1_norm = np.sqrt(np.dot(vector_1, vector_1))\n",
    "    vector_2_norm = np.sqrt(np.dot(vector_2, vector_2))\n",
    "\n",
    "    if vector_1_norm == 0 or vector_2_norm == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    cosine_sim = dot_prob/(vector_1_norm*vector_2_norm)\n",
    "    return cosine_sim\n",
    "\n",
    "# 45 Degree Difference\n",
    "vector_1 = np.array([1, 0])\n",
    "vector_2 = np.array([math.sqrt(2)/2, math.sqrt(2)/2])\n",
    "print(\"Cosine Similarity: \" + str(cosine(vector_1, vector_2)))\n",
    "print(\"cos(45 Degrees):\" + str(math.cos(math.pi/4)))\n",
    "\n",
    "# 90 Degree Difference\n",
    "vector_1 = np.array([1, 0])\n",
    "vector_2 = np.array([0, 1])\n",
    "print(\"Cosine Similarity: \" + str(cosine(vector_1, vector_2)))\n",
    "print(\"cos(90 Degrees):\" + str(math.cos(math.pi/2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we must talk about another issue with cosine similarity, and that issue is its a similarity measurement and not a distance. With a distance we think of a small value as being closer or more similar; however, with cosine similarity larger values are more similar and smaller are less similar. On A side note cosine similarity is bound between -1 and 1. So we must make cosine similarity operate like a normal distance. To do this we simply do 1 minus the cosine similarity. Now we have cosine distance which is bound between 0 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.2928932188134524\n",
      "1-cos(45 Degrees):0.2928932188134524\n",
      "Cosine Similarity: 1.0\n",
      "1-cos(90 Degrees):0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "# Create Updated Function\n",
    "def cosine(vector_1, vector_2):\n",
    "    dot_prob = np.dot(vector_1, vector_2)\n",
    "    vector_1_norm = np.sqrt(np.dot(vector_1, vector_1))\n",
    "    vector_2_norm = np.sqrt(np.dot(vector_2, vector_2))\n",
    "\n",
    "    if vector_1_norm == 0 or vector_2_norm == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    cosine_sim = dot_prob/(vector_1_norm*vector_2_norm)\n",
    "    return 1-cosine_sim\n",
    "\n",
    "# 45 Degree Difference\n",
    "vector_1 = np.array([1, 0])\n",
    "vector_2 = np.array([math.sqrt(2)/2, math.sqrt(2)/2])\n",
    "print(\"Cosine Similarity: \" + str(cosine(vector_1, vector_2)))\n",
    "print(\"1-cos(45 Degrees):\" + str(1-math.cos(math.pi/4)))\n",
    "\n",
    "# 90 Degree Difference\n",
    "vector_1 = np.array([1, 0])\n",
    "vector_2 = np.array([0, 1])\n",
    "print(\"Cosine Similarity: \" + str(cosine(vector_1, vector_2)))\n",
    "print(\"1-cos(90 Degrees):\" + str(1-math.cos(math.pi/2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation(Pearson's Correlation)\n",
    "Correlation is the measurment of linear relationship between two vectors(variables). Correlation is bound between -1 and 1. 1 is an absolute postive correlation, 0 is no correlation, and -1 is an absolute negative correlation. For comparing two vectors, its best to think about correlation as comparing two signals. If they have similar shape they will have a higher correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Johnny/Code/k-nearest-neighbors/knn_env/lib/python3.5/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "# Create Two Simple Vectors(Distance 1)\n",
    "vector_1 = np.array([1, 1])\n",
    "vector_2 = np.array([1, 0])\n",
    "\n",
    "vector_1_centered = vector_1-np.mean(vector_1)\n",
    "vector_2_centered = vector_2-np.mean(vector_2)\n",
    "\n",
    "vector_1_stddev = np.sqrt(np.dot(vector_1_centered, vector_1_centered))\n",
    "vector_2_stddev = np.sqrt(np.dot(vector_2_centered, vector_2_centered))\n",
    "\n",
    "covariance = np.dot(vector_1_centered, vector_2_centered)\n",
    "\n",
    "correlation = covariance/(vector_1_stddev * vector_2_stddev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we have another \"gotcha\". If a vector has a standard deviation of zero(aka a vector with all the same values) then we end up with a divide by zero error. So how should we handle this? Will remember that correlation is a measurement of LINEAR relationship. Since one of the vectors isn't changing we cant make a determination if there is a relationship. So in this case we will assign it a correlation of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: -0.9999999999999998\n"
     ]
    }
   ],
   "source": [
    "vector_1_centered = vector_1-np.mean(vector_1)\n",
    "vector_2_centered = vector_2-np.mean(vector_2)\n",
    "\n",
    "vector_1_stddev = np.sqrt(np.dot(vector_1_centered, vector_1_centered))\n",
    "vector_2_stddev = np.sqrt(np.dot(vector_2_centered, vector_2_centered))\n",
    "\n",
    "covariance = np.dot(vector_1_centered, vector_2_centered)\n",
    "\n",
    "if vector_1_stddev == 0 or vector_2_stddev == 0:\n",
    "    correlation = 0\n",
    "else:\n",
    "    correlation = covariance/(vector_1_stddev * vector_2_stddev)\n",
    "    \n",
    "print(\"Correlation: \" + str(correlation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: 0.9999999999999998\n",
      "Correlation: 1.0\n",
      "Correlation: -0.9999999999999998\n",
      "Correlation: -0.13372386230821978\n"
     ]
    }
   ],
   "source": [
    "# Create Function\n",
    "def correlation(vector_1, vector_2):\n",
    "    vector_1_centered = vector_1-np.mean(vector_1)\n",
    "    vector_2_centered = vector_2-np.mean(vector_2)\n",
    "    vector_1_stddev = np.sqrt(np.dot(vector_1_centered, vector_1_centered))\n",
    "    vector_2_stddev = np.sqrt(np.dot(vector_2_centered, vector_2_centered))\n",
    "    covariance = np.dot(vector_1_centered, vector_2_centered)\n",
    "\n",
    "    if vector_1_stddev == 0 or vector_2_stddev == 0:\n",
    "        return 0\n",
    "    \n",
    "    correlation = covariance/(vector_1_stddev * vector_2_stddev)\n",
    "    return correlation\n",
    "\n",
    "# Exact Same\n",
    "vector_1 = np.array([1, 0])\n",
    "vector_2 = np.array([1, 0])\n",
    "print(\"Correlation: \" + str(correlation(vector_1, vector_2)))\n",
    "\n",
    "# Same But Scaled\n",
    "vector_1 = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "vector_2 = np.array([2, 4, 6, 8, 10, 12, 14, 16])\n",
    "print(\"Correlation: \" + str(correlation(vector_1, vector_2)))\n",
    "\n",
    "# Exact Same But Flipped\n",
    "vector_1 = np.array([1, 0])\n",
    "vector_2 = np.array([-1, 0])\n",
    "print(\"Correlation: \" + str(correlation(vector_1, vector_2)))\n",
    "\n",
    "# Random Numbers\n",
    "vector_1 = np.random.normal(0, 1, 10)\n",
    "vector_2 = np.random.normal(0, 1, 10)\n",
    "print(\"Correlation: \" + str(correlation(vector_1, vector_2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So just like cosine similarity we run into the issue of correlation not being a true distance metric. We will do the exact same fix we did to cosine similarity, that is we will do 1 minus correlation. This is often referred to as pearson distance; we we will rename our method to reflect this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Updated Function\n",
    "def pearson(vector_1, vector_2):\n",
    "    vector_1_centered = vector_1-np.mean(vector_1)\n",
    "    vector_2_centered = vector_2-np.mean(vector_2)\n",
    "    vector_1_stddev = np.sqrt(np.dot(vector_1_centered, vector_1_centered))\n",
    "    vector_2_stddev = np.sqrt(np.dot(vector_2_centered, vector_2_centered))\n",
    "    covariance = np.dot(vector_1_centered, vector_2_centered)\n",
    "\n",
    "    if vector_1_stddev == 0 or vector_2_stddev == 0:\n",
    "        return 1\n",
    "    \n",
    "    correlation = covariance/(vector_1_stddev * vector_2_stddev)\n",
    "    return 1-correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi-Squared Distance\n",
    "In Chi-Squared Distance we will treat two vectors as a two-way contingency table and calculate a chi-squared statistic for the table. If the chi-squared statistic is near 0 we assume the two vectors are \"independent\" meaning their shape should be roughly similar; however, if the statistic is large there is some type of relationship meaning the shape is different for both vectors. This distance metric is only suitable for non-negative integers. You can imagine both vectors are a histogram and we are comparing their shapes to one another. You might see this in comparing frequency of words in a text documents.\n",
    "\n",
    "See: \"A Recent Advance in Data Analysis: Clustering Objects into Classes Characterized by Conjunctive Concepts\"\n",
    "      Michalski, R. S. et al.\n",
    "      http://mars.gmu.edu/jspui/handle/1920/1556?show=full\n",
    "See: https://stats.stackexchange.com/questions/184101/comparing-two-histograms-using-chi-square-distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-Square: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Johnny/Code/k-nearest-neighbors/knn_env/lib/python3.5/site-packages/ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in reciprocal\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "vector_1 = np.array([1., 0.])\n",
    "vector_2 = np.array([1., 0.])\n",
    "\n",
    "col_sums = vector_1 + vector_2\n",
    "col_sums_recip = np.reciprocal(col_sums)\n",
    "\n",
    "rel_freq_vector_1 = vector_1/np.sum(vector_1)\n",
    "rel_freq_vector_2 = vector_2/np.sum(vector_2)\n",
    "\n",
    "diff_rel_freq = rel_freq_vector_1-rel_freq_vector_2\n",
    "diff_rel_freq_square = np.square(diff_rel_freq)\n",
    "\n",
    "chisqr = np.sqrt(np.dot(col_sums_recip, diff_rel_freq_square))\n",
    "\n",
    "print(\"Chi-Square: \" + str(chisqr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First \"gotcha\" for this metric. If the sum of the two vectors end up with with a element that is 0 we get a divide by zero error when finding reciprocals. When this occurs we should not allow that element to contribute to the chi-squared statistic, so we will let reciprocal be equal to zero in that case. We can use the where argument of reciprocal() to skip finding the reciprocal of a number that is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-Square: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Johnny/Code/k-nearest-neighbors/knn_env/lib/python3.5/site-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "col_sums = vector_1 + vector_2\n",
    "col_sums_recip = np.reciprocal(col_sums, where=(col_sums!=0.0))    \n",
    "rel_freq_vector_1 = vector_1/np.sum(vector_1)\n",
    "rel_freq_vector_2 = vector_2/np.sum(vector_2)\n",
    "diff_rel_freq = rel_freq_vector_1-rel_freq_vector_2\n",
    "diff_rel_freq_square = np.square(diff_rel_freq)\n",
    "chisqr = np.dot(col_sums_recip, diff_rel_freq_square)\n",
    "\n",
    "print(\"Chi-Square: \" + str(chisqr))\n",
    "\n",
    "vector_1 = np.array([0., 0.])\n",
    "vector_2 = np.array([1., 0.])\n",
    "\n",
    "col_sums = vector_1 + vector_2\n",
    "col_sums_recip = np.reciprocal(col_sums, where=(col_sums!=0.0))    \n",
    "rel_freq_vector_1 = vector_1/np.sum(vector_1)\n",
    "rel_freq_vector_2 = vector_2/np.sum(vector_2)\n",
    "diff_rel_freq = rel_freq_vector_1-rel_freq_vector_2\n",
    "diff_rel_freq_square = np.square(diff_rel_freq)\n",
    "chisqr = np.sqrt(np.dot(col_sums_recip, diff_rel_freq_square))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One final \"gotcha\". If we have a vector of all zeros no statistic can be calculated, so we will return nan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-Square: nan\n",
      "Chi-Square: 0.0\n",
      "Chi-Square: 0.0\n",
      "Chi-Square: 0.12171612389003691\n",
      "Chi-Square: 0.019821345298596107\n"
     ]
    }
   ],
   "source": [
    "# Create Updated Function\n",
    "def chisqr(vector_1, vector_2):\n",
    "    col_sums = vector_1 + vector_2\n",
    "    col_sums_recip = np.reciprocal(col_sums, where=(col_sums!=0.0)) \n",
    "    vector_1_sum =  np.sum(vector_1)\n",
    "    vector_2_sum =  np.sum(vector_2)\n",
    "    \n",
    "    if vector_1_sum==0. or vector_2_sum==0.:\n",
    "        return np.nan\n",
    "    \n",
    "    rel_freq_vector_1 = vector_1/vector_1_sum\n",
    "    rel_freq_vector_2 = vector_2/vector_2_sum\n",
    "    diff_rel_freq = rel_freq_vector_1-rel_freq_vector_2\n",
    "    diff_rel_freq_square = np.square(diff_rel_freq)\n",
    "    chisqr = np.sqrt(np.dot(col_sums_recip, diff_rel_freq_square))\n",
    "    return chisqr\n",
    "\n",
    "vector_1 = np.array([0., 0.])\n",
    "vector_2 = np.array([1., 0.])\n",
    "print(\"Chi-Square: \" + str(chisqr(vector_1, vector_2)))\n",
    "\n",
    "# Exact Same\n",
    "vector_1 = np.array([1.0, 0.0])\n",
    "vector_2 = np.array([1.0, 0.0])\n",
    "print(\"Chi-Square: \" + str(chisqr(vector_1, vector_2)))\n",
    "\n",
    "# Same But Scaled\n",
    "vector_1 = np.array([1., 2., 3., 4., 5., 6., 7., 8.])\n",
    "vector_2 = np.array([2., 4., 6., 8., 10., 12., 14., 16.])\n",
    "print(\"Chi-Square: \" + str(chisqr(vector_1, vector_2)))\n",
    "    \n",
    "vector_1 = np.array([5.0, 10.0])\n",
    "vector_2 = np.array([10.0, 5.0])\n",
    "print(\"Chi-Square: \" + str(chisqr(vector_1, vector_2)))\n",
    "\n",
    "vector_1 = np.array([43.,44.,87.])\n",
    "vector_2 = np.array([9.,4.,13.])\n",
    "print(\"Chi-Square: \" + str(chisqr(vector_1, vector_2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hamming Distance\n",
    "Hamming distance is another simple distance metric. It is simply the count of discrepancies(non-equal values) between the two vectors. It is useful when comparing strings and bit strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming Distance: 0\n",
      "Hamming Distance: 6\n",
      "Hamming Distance: 3\n"
     ]
    }
   ],
   "source": [
    "vector_1 = np.array([1, 0, 0, 1, 0, 1])\n",
    "vector_2 = np.array([1, 0, 0, 1, 0, 1])\n",
    "\n",
    "distance = np.sum(vector_1 != vector_2)\n",
    "print(\"Hamming Distance: \" + str(distance))\n",
    "\n",
    "vector_1 = np.array([0, 0, 0, 0, 0, 0])\n",
    "vector_2 = np.array([1, 1, 1, 1, 1, 1])\n",
    "\n",
    "distance = np.sum(vector_1 != vector_2)\n",
    "print(\"Hamming Distance: \" + str(distance))\n",
    "\n",
    "vector_1 = np.array([1, 1, 1, 0, 0, 0])\n",
    "vector_2 = np.array([1, 1, 1, 1, 1, 1])\n",
    "\n",
    "distance = np.sum(vector_1 != vector_2)\n",
    "print(\"Hamming Distance: \" + str(distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming Distance: 0\n",
      "Hamming Distance: 6\n",
      "Hamming Distance: 3\n"
     ]
    }
   ],
   "source": [
    "def hamming(vector_1, vector_2):\n",
    "    return np.sum(vector_1 != vector_2)\n",
    "\n",
    "vector_1 = np.array([1, 0, 0, 1, 0, 1])\n",
    "vector_2 = np.array([1, 0, 0, 1, 0, 1])\n",
    "print(\"Hamming Distance: \" + str(hamming(vector_1, vector_2)))\n",
    "\n",
    "vector_1 = np.array([0, 0, 0, 0, 0, 0])\n",
    "vector_2 = np.array([1, 1, 1, 1, 1, 1])\n",
    "print(\"Hamming Distance: \" + str(hamming(vector_1, vector_2)))\n",
    "\n",
    "vector_1 = np.array([1, 1, 1, 0, 0, 0])\n",
    "vector_2 = np.array([1, 1, 1, 1, 1, 1])\n",
    "print(\"Hamming Distance: \" + str(hamming(vector_1, vector_2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knn_env_Python_3",
   "language": "python",
   "name": "knn_env_python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}